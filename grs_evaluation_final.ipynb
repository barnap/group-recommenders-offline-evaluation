{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd136c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tourism_dataset'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'preprocessed_tourism_dataset'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import settings.config as cfg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "preprocessed_dataset_folder = cfg.preprocessed_dataset_folder\n",
    "individual_rs_strategy = cfg.individual_rs_strategy\n",
    "aggregation_strategies = cfg.aggregation_strategies\n",
    "recommendations_number = cfg.recommendations_number\n",
    "individual_rs_validation_folds_k = cfg.individual_rs_validation_folds_k\n",
    "group_rs_evaluation_folds_k = cfg.group_rs_evaluation_folds_k\n",
    "evaluation_strategy = cfg.evaluation_strategy\n",
    "metrics = cfg.metrics\n",
    "evaluation_ground_truth = cfg.evaluation_ground_truth\n",
    "group_types = cfg.group_types\n",
    "\n",
    "display(cfg.dataset_folder,cfg.preprocessed_dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b49287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ratings_df = pd.read_csv(preprocessed_dataset_folder+\"/ratings.csv\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "group_composition = pickle.load(open(preprocessed_dataset_folder+\"/group_composition.pkl\", \"rb\"))\n",
    "len(group_composition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5213d31",
   "metadata": {},
   "source": [
    "## Train individual RS / Prepare groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a07749be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-17 21:40:43.460838 Select ground-truth for tourism dataset\n",
      "2022-09-17 21:40:43.463416 Done!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from individual_rs.individual_rs import IndividualRS\n",
    "from utils.utility_functions import create_per_user_group_choices\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# General pipeline\n",
    "\n",
    "# creating train-test folds\n",
    "# split stratified on the users \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import itertools\n",
    "\n",
    "if group_types == \"SYNTHETIC\":\n",
    "    print(datetime.now(), \"Creating folds\")\n",
    "    # skf = StratifiedKFold(n_splits=group_rs_evaluation_folds_k, random_state=None, shuffle=True)\n",
    "    skf = StratifiedKFold(n_splits=group_rs_evaluation_folds_k, random_state=42, shuffle=True)\n",
    "\n",
    "    print(datetime.now(), \"Folds created!\")\n",
    "    current_fold = 0\n",
    "    for train_index, test_index in skf.split(ratings_df, ratings_df['user']):\n",
    "        print(\">>> Start processing fold: Train\", len(train_index), \"Test:\", len(test_index))\n",
    "\n",
    "        # split train and test df\n",
    "        train_df = ratings_df.iloc[train_index]\n",
    "        test_df = ratings_df.iloc[test_index]\n",
    "\n",
    "        # getting user-items pairs in the training set\n",
    "        train_set_pairs = set(list(zip(train_df['user'].values,train_df['item'].values)))\n",
    "\n",
    "        # create test_complete_df with all the possible user-items pairs in the test_df\n",
    "        user_set = set(test_df['user'].values)\n",
    "        item_set = set(test_df['item'].values)\n",
    "        all_ui_values = list(itertools.product(user_set, item_set))\n",
    "        test_pred_df = pd.DataFrame(all_ui_values, columns=['user', 'item'])\n",
    "\n",
    "    #     print(datetime.now(), \"Extended test df\")\n",
    "    #     display(test_pred_df)\n",
    "\n",
    "        print(datetime.now(), \"Train individual RS and get predictions\")\n",
    "        # train individual rs and get predictions\n",
    "        test_pred_df = IndividualRS.train_individual_rs_and_get_predictions(train_df, test_pred_df)\n",
    "\n",
    "        #correction for train set records (assuming repeated recommendations provide no value, therefore predicted_rating=0)\n",
    "        train_set_pairs = train_set_pairs.intersection(set(all_ui_values))\n",
    "        test_pred_df.set_index([\"user\",\"item\"], inplace=True)\n",
    "        test_pred_df.loc[train_set_pairs,\"predicted_rating\"] = 0.0\n",
    "        test_pred_df.reset_index(inplace=True)\n",
    "\n",
    "        path_to_fold = preprocessed_dataset_folder+\"/fold_\"+str(current_fold)\n",
    "\n",
    "        if not os.path.exists(path_to_fold):\n",
    "            os.mkdir(path_to_fold)\n",
    "\n",
    "        pickle.dump(train_df, open(path_to_fold+\"/train_df.pkl\", \"wb\"))\n",
    "        pickle.dump(test_df, open(path_to_fold+\"/test_df.pkl\", \"wb\"))\n",
    "        pickle.dump(test_pred_df, open(path_to_fold+\"/test_pred_df.pkl\", \"wb\"))\n",
    "\n",
    "        current_fold = current_fold + 1\n",
    "\n",
    "else:\n",
    "    print(datetime.now(), \"Select ground-truth for tourism dataset\")\n",
    "    if evaluation_ground_truth == \"GROUP_CHOICES\":\n",
    "        group_choices = pd.read_csv(preprocessed_dataset_folder+\"/group_choices.csv\")\n",
    "        ground_truth = create_per_user_group_choices(group_composition, group_choices)\n",
    "    elif evaluation_ground_truth == \"USER_SATISFACTION\":\n",
    "        group_choices = pd.read_csv(preprocessed_dataset_folder+\"/group_choices.csv\")\n",
    "        user_feedback = pd.read_csv(preprocessed_dataset_folder+\"/user_feedback.csv\")\n",
    "        ground_truth = create_per_user_satisfaction(group_composition, group_choices, user_feedback)\n",
    "    else:\n",
    "        ground_truth = ratings_df[['user', 'item', 'rating']]\n",
    "    \n",
    "    current_fold = 0\n",
    "    path_to_fold = preprocessed_dataset_folder+\"/fold_\"+str(current_fold)\n",
    "    if not os.path.exists(path_to_fold):\n",
    "        os.mkdir(path_to_fold)\n",
    "    pickle.dump(ground_truth, open(path_to_fold+\"/test_df.pkl\", \"wb\"))\n",
    "\n",
    "print(datetime.now(), \"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82f992",
   "metadata": {},
   "source": [
    "# Construct group recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5cda9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-17 21:40:48.928359 fold_0: Generate GRS for all the aggregation strategies and all the groups\n",
      "2022-09-17 21:40:49.433214 fold_0: Done\n"
     ]
    }
   ],
   "source": [
    "from utils.utility_functions import generate_group_recommendations_forall_groups\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "lst = os.listdir(preprocessed_dataset_folder)\n",
    "folds = [i for i in lst if (os.path.isdir(preprocessed_dataset_folder+\"/\"+i) and i.startswith(\"fold\"))]\n",
    "\n",
    "for f in folds:\n",
    "    current_fold = int(f.replace(\"fold_\",\"\"))\n",
    "    path_to_fold = preprocessed_dataset_folder+\"/\"+f\n",
    "    \n",
    "    if group_types == \"REAL\":\n",
    "        test_pred_df = ratings_df[['user', 'item', 'rating']]\n",
    "        test_pred_df.columns = ['user', 'item', 'predicted_rating']\n",
    "    else:\n",
    "        train_df = pickle.load(open(path_to_fold+\"/train_df.pkl\", \"rb\"))\n",
    "        test_df = pickle.load(open(path_to_fold+\"/test_df.pkl\", \"rb\"))\n",
    "        test_pred_df = pickle.load(open(path_to_fold+\"/test_pred_df.pkl\", \"rb\"))\n",
    "    \n",
    "\n",
    "    print(datetime.now(), f+\": Generate GRS for all the aggregation strategies and all the groups\")\n",
    "    # - generate the recommendations for all the aggregation strategies and all the groups\n",
    "    group_recommendations = generate_group_recommendations_forall_groups(test_pred_df, group_composition, cfg.recommendations_number)\n",
    "    print(datetime.now(), f+\": Done\")\n",
    "    \n",
    "    pickle.dump(group_recommendations, open(path_to_fold+\"/group_recommendations.pkl\", \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879b9ed",
   "metadata": {},
   "source": [
    "# Evaluate group recommendations\n",
    "### Define evaluation variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc5bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    " evaluation_variants = [\n",
    "     {\n",
    "         \"evaluation_strategy\": \"COUPLED\",\n",
    "         \"evaluation_ground_truth\": \"GROUP_CHOICES\",\n",
    "         \"binarize_feedback\": False,\n",
    "         \"inverse_propensity_debiasing\": False,\n",
    "         \"feedback_polarity_debiasing\": 0.0\n",
    "     },\n",
    "     {\n",
    "         \"evaluation_strategy\": \"COUPLED\",\n",
    "         \"evaluation_ground_truth\": \"USER_RATINGS\",\n",
    "         \"binarize_feedback\": False,\n",
    "         \"inverse_propensity_debiasing\": False,\n",
    "         \"feedback_polarity_debiasing\": 0.0\n",
    "     },\n",
    "     {\n",
    "         \"evaluation_strategy\": \"COUPLED\",\n",
    "         \"evaluation_ground_truth\": \"USER_SATISFACTION\",\n",
    "         \"binarize_feedback\": False,\n",
    "         \"inverse_propensity_debiasing\": False,\n",
    "         \"feedback_polarity_debiasing\": 0.0\n",
    "     },\n",
    " ]\n",
    "#evaluation_variants = [\n",
    "#    {\n",
    "#        \"evaluation_strategy\": \"COUPLED\",\n",
    "#        \"binarize_feedback\": True,\n",
    "#        \"binarize_feedback_positive_threshold\": 4.0,\n",
    "#        \"inverse_propensity_debiasing\": False    \n",
    "#    },\n",
    "#    {\n",
    "#        \"evaluation_strategy\": \"COUPLED\",\n",
    "#        \"binarize_feedback\": True,\n",
    "#        \"binarize_feedback_positive_threshold\": 4.0,\n",
    "#        \"inverse_propensity_debiasing\": True,\n",
    "#        \"inverse_propensity_gamma\": 0.5\n",
    "#    },\n",
    "#    {\n",
    "#        \"evaluation_strategy\": \"COUPLED\",\n",
    "#        \"binarize_feedback\": False,\n",
    "#        \"inverse_propensity_debiasing\": False    \n",
    "#    },\n",
    "#    {\n",
    "#        \"evaluation_strategy\": \"COUPLED\",\n",
    "#        \"binarize_feedback\": False,\n",
    "#        \"inverse_propensity_debiasing\": True,\n",
    "#        \"inverse_propensity_gamma\": 0.5\n",
    "#    },    \n",
    "#]\n",
    "# evaluation_variants = [\n",
    "#     {\n",
    "#         \"evaluation_strategy\": \"DECOUPLED\",\n",
    "#         \"binarize_feedback\": True,\n",
    "#         \"binarize_feedback_positive_threshold\": 4.0,\n",
    "#         \"feedback_polarity_debiasing\": 0.0    \n",
    "#     },\n",
    "#     {\n",
    "#         \"evaluation_strategy\": \"DECOUPLED\",\n",
    "#         \"binarize_feedback\": True,\n",
    "#         \"binarize_feedback_positive_threshold\": 3.0,\n",
    "#         \"feedback_polarity_debiasing\": 0.0    \n",
    "#     },    \n",
    "#     {\n",
    "#         \"evaluation_strategy\": \"DECOUPLED\",\n",
    "#         \"binarize_feedback\": False,\n",
    "#         \"binarize_feedback_positive_threshold\": 4.0,\n",
    "#         \"feedback_polarity_debiasing\": 0.0\n",
    "#     },\n",
    "#     {\n",
    "#         \"evaluation_strategy\": \"DECOUPLED\",\n",
    "#         \"binarize_feedback\": False,\n",
    "#         \"binarize_feedback_positive_threshold\": 4.0,\n",
    "#         \"feedback_polarity_debiasing\": -2.0   \n",
    "#     },\n",
    "#     {\n",
    "#         \"evaluation_strategy\": \"DECOUPLED\",\n",
    "#         \"binarize_feedback\": False,\n",
    "#         \"binarize_feedback_positive_threshold\": 4.0,\n",
    "#         \"feedback_polarity_debiasing\": -4.0\n",
    "#     },    \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff23c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-17 21:40:56.907656 fold_0: Evaluate Group recommendations\n",
      "2022-09-17 21:41:43.042153 Fold Evaluation DONE\n",
      "2022-09-17 21:41:43.047638 fold_0: Evaluate Group recommendations\n",
      "2022-09-17 21:42:25.230090 Fold Evaluation DONE\n",
      "2022-09-17 21:42:25.239552 fold_0: Evaluate Group recommendations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from utils.utility_functions import calculate_inverse_propensity_score,calculate_inverse_propensity_score_user_normalization\n",
    "from utils.utility_functions import evaluate_group_recommendations_forall_groups\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lst = os.listdir(preprocessed_dataset_folder)\n",
    "folds = [i for i in lst if (os.path.isdir(preprocessed_dataset_folder+\"/\"+i) and i.startswith(\"fold\"))]\n",
    "\n",
    "for ev in evaluation_variants:\n",
    "    for i in ev.keys():\n",
    "        setattr(cfg, i, ev[i])\n",
    "\n",
    "    evaluations = list()\n",
    "    for f in folds:\n",
    "        current_fold = int(f.replace(\"fold_\",\"\"))\n",
    "        path_to_fold = preprocessed_dataset_folder+\"/\"+f\n",
    "        \n",
    "        if group_types == \"REAL\":\n",
    "            test_df = pickle.load(open(path_to_fold+\"/test_df.pkl\", \"rb\"))\n",
    "        else:\n",
    "            train_df = pickle.load(open(path_to_fold+\"/train_df.pkl\", \"rb\"))\n",
    "            test_df = pickle.load(open(path_to_fold+\"/test_df.pkl\", \"rb\"))\n",
    "            test_pred_df = pickle.load(open(path_to_fold+\"/test_pred_df.pkl\", \"rb\"))\n",
    "\n",
    "        group_recommendations = pickle.load(open(path_to_fold+\"/group_recommendations.pkl\", \"rb\"))\n",
    "\n",
    "        # - evaluate the recommendations\n",
    "        if cfg.evaluation_strategy == \"COUPLED\":\n",
    "            ground_truth = test_df\n",
    "        else:\n",
    "            ground_truth = test_pred_df.rename(columns={\"predicted_rating\": \"rating\"}, errors=\"raise\")\n",
    "\n",
    "        if cfg.inverse_propensity_debiasing == True and cfg.evaluation_strategy == \"COUPLED\":\n",
    "            propensity_per_item = calculate_inverse_propensity_score(ratings_df, train_df, cfg.inverse_propensity_gamma)\n",
    "            per_user_propensity_normalization_term = calculate_inverse_propensity_score_user_normalization(propensity_per_item, test_df)\n",
    "        else:\n",
    "            #dummies to simplify downstream code\n",
    "            propensity_per_item = pd.Series({\"propensity_score\":1.0})\n",
    "            per_user_propensity_normalization_term = None\n",
    "\n",
    "        print(datetime.now(), f+\": Evaluate Group recommendations\")\n",
    "        fold_group_evaluations = evaluate_group_recommendations_forall_groups(\n",
    "            ground_truth, \n",
    "            group_recommendations, \n",
    "            group_composition, \n",
    "            propensity_per_item, \n",
    "            per_user_propensity_normalization_term,\n",
    "            current_fold)\n",
    "        print(datetime.now(), \"Fold Evaluation DONE\")\n",
    "        #display(fold_group_evaluations)\n",
    "\n",
    "        evaluations = evaluations + fold_group_evaluations\n",
    "        #current_fold = current_fold + 1\n",
    "    eval_df = pd.DataFrame.from_records(evaluations)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    path_to_eval_folder = preprocessed_dataset_folder+\"/eval_\"+str(time.time())\n",
    "    eval_parameters = {key:cfg.__dict__[key] for key in cfg.__dict__.keys() if not (key.startswith('__') or key.startswith('_'))}\n",
    "\n",
    "    if not os.path.exists(path_to_eval_folder):\n",
    "        os.mkdir(path_to_eval_folder)\n",
    "        \n",
    "    \n",
    "    pickle.dump(eval_parameters, open(path_to_eval_folder+\"/eval_parameters.pkl\", \"wb\"))    \n",
    "    pickle.dump(eval_df, open(path_to_eval_folder+\"/eval_df.pkl\", \"wb\"))\n",
    "    pickle.dump(group_composition, open(path_to_eval_folder+\"/group_composition.pkl\", \"wb\"))\n",
    "    #for human readability\n",
    "    with open(path_to_eval_folder+\"/eval_parameters.json\" , \"w\" ) as write:\n",
    "        json.dump( eval_parameters , write )        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1239899",
   "metadata": {},
   "source": [
    "# Show how individual evaluation runs differs from each other\n",
    "- only show parameters with non-uniform values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c23aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = os.listdir(preprocessed_dataset_folder)\n",
    "results = [i for i in lst if (os.path.isdir(preprocessed_dataset_folder+\"/\"+i) and i.startswith(\"eval\"))]\n",
    "list(enumerate(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adbec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = os.listdir(preprocessed_dataset_folder)\n",
    "results = [i for i in lst if (os.path.isdir(preprocessed_dataset_folder+\"/\"+i) and i.startswith(\"eval\"))]\n",
    "\n",
    "eval_params_list = []\n",
    "for r in results:\n",
    "    path_to_eval_folder = preprocessed_dataset_folder + \"/\" + r\n",
    "    eval_params = pickle.load(open(path_to_eval_folder+\"/eval_parameters.pkl\", \"rb\"))\n",
    "    eval_params_list.append(eval_params)\n",
    "\n",
    "parameters_df = pd.DataFrame(eval_params_list)\n",
    "parameters_df.index = results\n",
    "keep_col = []\n",
    "parameters_df = parameters_df.astype(str)\n",
    "for c in parameters_df.columns:\n",
    "    if len(parameters_df.loc[:,c].unique()) > 1:\n",
    "        keep_col.append(True)\n",
    "    else:\n",
    "        keep_col.append(False)\n",
    "parameters_df_relevant = parameters_df.loc[:,keep_col]  \n",
    "parameters_df_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad51f5",
   "metadata": {},
   "source": [
    "# Visualize results from individual evaluation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select which evaluation run to display\n",
    "res_folder = parameters_df.index[0]\n",
    "path_to_eval_folder = preprocessed_dataset_folder + \"/\" + res_folder\n",
    "\n",
    "eval_parameters = pickle.load(open(path_to_eval_folder+\"/eval_parameters.pkl\", \"rb\"))\n",
    "eval_df = pickle.load(open(path_to_eval_folder+\"/eval_df.pkl\", \"rb\"))\n",
    "group_composition = pickle.load(open(path_to_eval_folder+\"/group_composition.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add / remove metrics considered for evaluation\n",
    "metric_whitelist = [\"NDCG\",\"DCG\",\"DFH\",\"Recall\",\"BoundedRecall\",\"zRecall\",\"MRR\"]\n",
    "eval_df = eval_df.loc[eval_df.metric.isin(metric_whitelist)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c80741",
   "metadata": {},
   "source": [
    "### Depict overall (mean) results across all groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5079baf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_group_res = eval_df.groupby(['metric', 'aggr_metric', 'aggregation_strategy']).mean().reset_index()\n",
    "\n",
    "g = sns.catplot(data=eval_df, row=\"metric\", col=\"aggr_metric\", \n",
    "                   x=\"aggregation_strategy\", y=\"value\", sharex=False, sharey=False,\n",
    "                   kind=\"bar\",  height=3.5, aspect=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268db036",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_composition_DF = pd.DataFrame(group_composition).T\n",
    "eval_df_with_group_info = group_composition_DF.join(eval_df.set_index(\"group_id\"))\n",
    "eval_df_with_group_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ffdf99",
   "metadata": {},
   "source": [
    "### Metric variance per group size; keeping the aggregation strategies visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0263033",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"NDCG\"\n",
    "g = sns.catplot(data=eval_df_with_group_info.loc[eval_df_with_group_info.metric==metric], col=\"group_size\", row=\"aggr_metric\", \n",
    "                   x=\"aggregation_strategy\", y=\"value\", sharex=False, sharey=\"row\",\n",
    "                   kind=\"bar\",  height=3.5, aspect=1.2)\n",
    "plt.suptitle(metric)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef203c",
   "metadata": {},
   "source": [
    "### Metric variance per group size; merging all aggregation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ffa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=eval_df_with_group_info, row=\"metric\", col=\"aggr_metric\", \n",
    "                   x=\"group_size\", y=\"value\", sharex=False, sharey=False,\n",
    "                   kind=\"bar\",  height=3.5, aspect=1.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f17da",
   "metadata": {},
   "source": [
    "### Metric variance per group size and group type; merging all aggregation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12788f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"NDCG\"\n",
    "g = sns.catplot(data=eval_df_with_group_info.loc[eval_df_with_group_info.metric==metric], hue=\"group_similarity\", col=\"aggr_metric\", \n",
    "                   x=\"group_size\", y=\"value\", sharex=False, sharey=\"row\",\n",
    "                   kind=\"bar\",  height=3.5, aspect=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e0d581",
   "metadata": {},
   "source": [
    "### Depict distribution of per-group values for all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f0f09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.catplot(data=eval_df, row=\"metric\", col=\"aggr_metric\", \n",
    "                   x=\"aggregation_strategy\", y=\"value\", sharex=False, sharey=False,\n",
    "                   kind=\"boxen\",  height=3.5, aspect=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_folds = eval_df.groupby(['metric', 'aggr_metric', 'aggregation_strategy', 'group_id']).mean()\n",
    "#display(group_folds['value'].reset_index().sort_values(by='group_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cc20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_df.groupby(['metric', 'aggr_metric', 'aggregation_strategy']).mean().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aca11d",
   "metadata": {},
   "source": [
    "# Compare results of multiple evaluation runs\n",
    "### Collect all results and extend eval_df with relevant evaluation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ab56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all evaluation runs specified in valid_rows\n",
    "def get_valid_evaluation_data(valid_rows):\n",
    "    eval_df_list = []\n",
    "    for res in valid_rows:\n",
    "        path_to_eval_folder = preprocessed_dataset_folder + \"/\" + res\n",
    "        eval_df = pickle.load(open(path_to_eval_folder+\"/eval_df.pkl\", \"rb\"))    \n",
    "        for c in parameters_df_relevant.columns:\n",
    "            eval_df[c]=parameters_df_relevant.loc[res,c]\n",
    "        eval_df_list.append(eval_df)\n",
    "\n",
    "    eval_df_concat = pd.concat(eval_df_list)\n",
    "    return eval_df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only those evaluation runs we consider interesting\n",
    "valid_rows = parameters_df_relevant.index[5:]\n",
    "parameters_df_relevant.loc[valid_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_concat = get_valid_evaluation_data(valid_rows)\n",
    "eval_df_concat.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b45e1",
   "metadata": {},
   "source": [
    "### Coupled evaluation: effect of feedback binarization and inverse propensity debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"DCG\"\n",
    "g = sns.catplot(data=eval_df_concat.loc[eval_df_concat.metric==metric], x=\"binarize_feedback\", \n",
    "                col=\"aggr_metric\", \n",
    "                hue=\"aggregation_strategy\", y=\"value\", sharex=\"col\", sharey=False,\n",
    "                kind=\"bar\",  height=3.5, aspect=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125931ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"DCG\"\n",
    "g = sns.catplot(data=eval_df_concat.loc[eval_df_concat.metric==metric],  \n",
    "                col=\"aggr_metric\", hue=\"binarize_feedback\", \n",
    "                x=\"aggregation_strategy\", y=\"value\", sharex=False, sharey=False,\n",
    "                kind=\"bar\",  height=3.5, aspect=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26229d3a",
   "metadata": {},
   "source": [
    "### Decoupled evaluation: effect of polarity debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_rows = parameters_df_relevant.index[2:5]\n",
    "eval_df_concat = get_valid_evaluation_data(valid_rows)\n",
    "parameters_df_relevant.loc[valid_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_df_concat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c193d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"NDCG\"\n",
    "g = sns.catplot(data=eval_df_concat.loc[eval_df_concat.metric==metric], x=\"feedback_polarity_debiasing\", \n",
    "                col=\"aggr_metric\", \n",
    "                hue=\"aggregation_strategy\", y=\"value\", sharex=False, sharey=\"row\",\n",
    "                kind=\"bar\",  height=3.5, aspect=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1104aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_rows = parameters_df_relevant.index[0:2]\n",
    "eval_df_concat = get_valid_evaluation_data(valid_rows)\n",
    "parameters_df_relevant.loc[valid_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c012a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"NDCG\"\n",
    "g = sns.catplot(data=eval_df_concat.loc[eval_df_concat.metric==metric], x=\"binarize_feedback_positive_threshold\", \n",
    "                col=\"aggr_metric\", \n",
    "                hue=\"aggregation_strategy\", y=\"value\", sharex=False, sharey=False,\n",
    "                kind=\"bar\",  height=3.5, aspect=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bba8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
